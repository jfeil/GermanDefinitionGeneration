{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-16T19:43:06.917900Z",
     "start_time": "2024-07-16T19:43:06.577699Z"
    }
   },
   "source": "from evaluation import load_model",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:44:40.186707Z",
     "start_time": "2024-07-16T19:44:30.234195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "selected_model = \"32010753b6f3476eb51f55e6898a44e7\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, tokenizer, prompt_pattern = load_model(selected_model, device=device)"
   ],
   "id": "fbd54da16321d328",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36mhqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jfeil/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:48:45.238661Z",
     "start_time": "2024-07-16T19:48:44.277605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.model_training.datasets.experiments_sanitize.complete_sanitization import DefinitionTestSet, DefaultTrainValSet\n",
    "DefinitionTestSet.prompt_pattern = prompt_pattern\n",
    "dataset_test = DefinitionTestSet.create_dataset(tokenizer, shuffle=True, seed=42, subset_test=-1)"
   ],
   "id": "23afcc12fce82a13",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:26:27.927244Z",
     "start_time": "2024-07-16T20:26:27.921231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=device)"
   ],
   "id": "38f1adef0eff5b41",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:31:41.097927Z",
     "start_time": "2024-07-16T20:31:11.552286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_generator():\n",
    "    for item in dataset_test:\n",
    "        yield item[\"prompt\"]\n",
    "        \n",
    "strategies = [\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"repetition_penalty\": 1.1},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"repetition_penalty\": 1.2},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"repetition_penalty\": 1.3},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"repetition_penalty\": 1.4},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"repetition_penalty\": 1.5},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"repetition_penalty\": 1.8},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"repetition_penalty\": 2.0, \"temperature\": 1.2},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 0.5},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 1.1},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 1.2},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 1.3},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 1.4},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 1.5},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 1.8},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 2.0, \"temperature\": 1.2},\n",
    "        {\"name\": \"Encoder Repetition penalty\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"encoder_repetition_penalty\": 0.5},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 0.1},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 0.3},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 0.5},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 0.7},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 0.9},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 1.1},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 1.2},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 1.4},\n",
    "        {\"name\": \"Temperature\", \"do_sample\": True, \"num_beams\": 5, \"max_new_tokens\": 50, \"temperature\": 1.5},\n",
    "    {\"name\": \"No repeat NGRAM\", \"no_repeat_ngram_size\": 2}\n",
    "\n",
    "]\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "    test_predictions = {\n",
    "        'title': dataset_test['title'],\n",
    "        'context_sentence': dataset_test['context_sentence'],\n",
    "        'context_word': dataset_test['context_word'],\n",
    "        'gt': dataset_test['gt'],\n",
    "        'prediction': []\n",
    "    }\n",
    "    \n",
    "    used_strategy = dict(strategy)\n",
    "    used_strategy.pop(\"name\")\n",
    "    for out in pipe(data_generator(), batch_size=32, **used_strategy):\n",
    "        assert len(out) == 1\n",
    "        test_predictions['prediction'].append(out[0][\"generated_text\"])\n",
    "        print(f\"{i}: {out[0]['generated_text']}\")\n",
    "        break"
   ],
   "id": "3c45152381fbf3ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Sport: Ernüchterung eines Spielers, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft\n",
      "1: Sport, Fußball: Zustand, in dem etwas (beispielsweise ein Spiel, ein Spiel, ein Spiel, ein Spiel, einen Wettkampf, einen Wettkampf, einen Wettkampf, einen Wettkampf, einen Wett\n",
      "2: Sport: Ernüchterung von etwas\n",
      "3: Sport: Ernüchterung eines Spielers, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft\n",
      "4: Sport, Fußball: Ernüchterung\n",
      "5: bildungssprachlich: Ernüchterung\n",
      "6: Sport: etwas, das in der Folge eines Wettkampfes zustande kommt\n",
      "7: Sport, Fußball: das Erzeugen eines Spielers, eines Spielers, eines Spielers, eines Spielers, eines Spielers, eines Spielers, eines Spielers, eines Spielers oder eines Spieler\n",
      "8: Sport, Fussball: Ernüchterung eines Spielers, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft,\n",
      "9: Sport, Fußball: Ernüchterung von etwas\n",
      "10: Sport, Fußball: Ernüchterung einer Mannschaft\n",
      "11: erfolgreiche Ernüchterung einer Mannschaft, beispielsweise einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft,\n",
      "12: Sport, Fußball: erneute Ernüchterung\n",
      "13: erfolgreiche Ernüchterung von etwas (beispielsweise von einer anderen Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, \n",
      "14: natürliche Erwartung einer Person\n",
      "15: Sport, Fußball: Verunreinigung eines Spielers oder Mannschafts, die in der Form eines Fußballspielers (einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer\n",
      "16: Sport, Fußball: Ernüchterung eines Spielers, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, \n",
      "17: Sport, Fußball: Ernüchterung eines Spielers, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, einer Mannschaft, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 45\u001B[0m\n\u001B[1;32m     43\u001B[0m used_strategy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(strategy)\n\u001B[1;32m     44\u001B[0m used_strategy\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m out \u001B[38;5;129;01min\u001B[39;00m pipe(data_generator(), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mused_strategy):\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     47\u001B[0m     test_predictions[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[1;32m    124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator)\n\u001B[0;32m--> 125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;66;03m# Try to infer the size of the batch\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/transformers/pipelines/base.py:1161\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1159\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1160\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1161\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1162\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:191\u001B[0m, in \u001B[0;36mText2TextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m     in_b, input_length \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mshape(model_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_inputs(\n\u001B[1;32m    187\u001B[0m     input_length,\n\u001B[1;32m    188\u001B[0m     generate_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmin_length),\n\u001B[1;32m    189\u001B[0m     generate_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_length),\n\u001B[1;32m    190\u001B[0m )\n\u001B[0;32m--> 191\u001B[0m output_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    192\u001B[0m out_b \u001B[38;5;241m=\u001B[39m output_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/transformers/generation/utils.py:2008\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2000\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2001\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2002\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   2003\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2004\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2005\u001B[0m     )\n\u001B[1;32m   2007\u001B[0m     \u001B[38;5;66;03m# 14. run beam sample\u001B[39;00m\n\u001B[0;32m-> 2008\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2009\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2010\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeam_scorer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2011\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2012\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2013\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2014\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2015\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2016\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2017\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2019\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGROUP_BEAM_SEARCH:\n\u001B[1;32m   2020\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[1;32m   2021\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[1;32m   2022\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   2023\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2029\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   2030\u001B[0m     )\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/transformers/generation/utils.py:3229\u001B[0m, in \u001B[0;36mGenerationMixin._beam_search\u001B[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001B[0m\n\u001B[1;32m   3226\u001B[0m next_tokens \u001B[38;5;241m=\u001B[39m next_tokens \u001B[38;5;241m%\u001B[39m vocab_size\n\u001B[1;32m   3228\u001B[0m \u001B[38;5;66;03m# stateless\u001B[39;00m\n\u001B[0;32m-> 3229\u001B[0m beam_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mbeam_scorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3230\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3231\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_token_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3232\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3233\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3234\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3235\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3236\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeam_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeam_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3237\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_prompt_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_prompt_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3238\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3240\u001B[0m beam_scores \u001B[38;5;241m=\u001B[39m beam_outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_beam_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   3241\u001B[0m beam_next_tokens \u001B[38;5;241m=\u001B[39m beam_outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_beam_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/transformers/generation/beam_search.py:273\u001B[0m, in \u001B[0;36mBeamSearchScorer.process\u001B[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001B[0m\n\u001B[1;32m    271\u001B[0m batch_beam_idx \u001B[38;5;241m=\u001B[39m batch_idx \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_size \u001B[38;5;241m+\u001B[39m next_index\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# add to generated hypotheses if end of sentence\u001B[39;00m\n\u001B[0;32m--> 273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (eos_token_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[43mnext_token\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43meos_token_id\u001B[49m):\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;66;03m# if beam_token does not belong to top num_beams tokens, it should not be added\u001B[39;00m\n\u001B[1;32m    275\u001B[0m     is_beam_token_worse_than_top_num_beams \u001B[38;5;241m=\u001B[39m beam_token_rank \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_size\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_beam_token_worse_than_top_num_beams:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.12/envs/MasterThesis-NLP/lib/python3.10/site-packages/torch/_tensor.py:1091\u001B[0m, in \u001B[0;36mTensor.__contains__\u001B[0;34m(self, element)\u001B[0m\n\u001B[1;32m   1086\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__contains__\u001B[39m, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, element)\n\u001B[1;32m   1087\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m   1088\u001B[0m     element, (torch\u001B[38;5;241m.\u001B[39mTensor, Number, torch\u001B[38;5;241m.\u001B[39mSymInt, torch\u001B[38;5;241m.\u001B[39mSymFloat, torch\u001B[38;5;241m.\u001B[39mSymBool)\n\u001B[1;32m   1089\u001B[0m ):\n\u001B[1;32m   1090\u001B[0m     \u001B[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001B[39;00m\n\u001B[0;32m-> 1091\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m(\u001B[49m\u001B[43melement\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43many\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mitem()  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[1;32m   1093\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1094\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(element)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1095\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:32:16.013856Z",
     "start_time": "2024-07-16T20:32:15.984102Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_test['prompt'][0]",
   "id": "cf698c711e885f95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nach den letzten großartigen Erfolgen des FC Bayern erfolgte bei der Heimniederlage die Ernüchterung. Was ist die Definition von Ernüchterung.? '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d88a51435a7dcea5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
